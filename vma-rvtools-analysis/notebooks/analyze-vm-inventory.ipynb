{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa7e99f-0c30-40e7-be7c-37f29aa9007f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Python 3 notebook explores the exported VMWare inventory generated by [RVTools](https://www.robware.net/) and generates various statistical analyses of the data for assessing the scale, complexity, and other characteristics.\n",
    "\n",
    "The analysis is directed towards understanding the feasibility of migrating these VMWare virtual machines to Red Hat's OpenShift Virtualization Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e010991-ca96-43de-af5f-cd22bfdbd72e",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "This section configures the script and the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Required Packages\n",
    "\n",
    "This script uses `pandas` and `numpy` for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the VM inventory generated by rvtools\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set up a few options and other configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to prevent line wrapping\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Set up the data directory\n",
    "First, set up the directory where the RVTools Excel data files will be stored. This folder must also contain an index.xlsx file (refer to the provided template for the expected format). The index.xlsx file should list the valid RVTools Excel file names along with their corresponding vCenter instances. This script assumes that there is one Excel file per vCenter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current script directory\n",
    "current_dir = \".\"\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "DATA_DIR = os.path.join(current_dir, '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration parameters\n",
    "\n",
    "# The index file\n",
    "INDEX_FILENAME = \"index.xlsx\"\n",
    "INDEX_FILEPATH = os.path.join(DATA_DIR, INDEX_FILENAME)\n",
    "INDEX_SHEETNAME = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "This section defines a few functions for decomposing the analysis code. It reads multiple Excel .xlsx files exported from RVtools software and returns a dictionary with filenames as keys and a dictionary of DataFrames (one for each sheet) as values.\n",
    "\n",
    "* The `index.xlsx` and `index_template.xlsx` files are explicitly ignored.\n",
    "\n",
    "Parameters:\n",
    "* directory (str): The directory containing the Excel files.\n",
    "* filenames_to_process (list): A list of filenames to process.\n",
    "\n",
    "Returns:\n",
    "* dict: A dictionary containing the DataFrames from each Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rvtools_excel_files(directory, filenames_to_process):\n",
    "    rvtools_data = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        # Only process files listed in the index file.\n",
    "        filename_base, _ = os.path.splitext(filename)\n",
    "        if filename_base not in filenames_to_process: continue\n",
    "        if filename in ['index.xlsx', 'index_template.xlsx']: continue\n",
    "\n",
    "        if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            excel_data = pd.read_excel(filepath, sheet_name=None)\n",
    "            rvtools_data[filename] = excel_data\n",
    "\n",
    "    return rvtools_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Read, Clean and Filter the RVTools data\n",
    "\n",
    "This section reads the `RVTools` exported files. It uses an `index.xlsx` metadata file to identify the\n",
    "in-scope `vCenter` instances for the migration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Read the index metadata file\n",
    "\n",
    "First read the index file to determine the RVTools files to process. This file contains additional metadata,\n",
    "including the `vCenter` instances In-Scope, which can be customized to process specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First read the index Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Count occurrences of each vCenter\n",
    "pivot_table = inscope_df.groupby('vCenter').size().reset_index(name='Count')\n",
    "\n",
    "# Print in a structured table format\n",
    "print(\"üîç In-Scope vCenter Instances:\")\n",
    "print(pivot_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00709fbf-13ea-49e8-af60-8fac524d0244",
   "metadata": {},
   "source": [
    "### Read the RVTool Exported Spreadsheets\n",
    "\n",
    "Read all the RVTools exported files available in the `data` directory. The data will be read into a dictionary,\n",
    "with one entry per file, where the key is the filename, and the value is **another** nested dictionary with the spreadsheet's\n",
    "sheet name as the key, and a `DataFrame` containing the sheets values.\n",
    "\n",
    "The files **need** to be in the Microsoft `xlsx` format.\n",
    "\n",
    "**Note**: This step will take a few minutes to complete. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fce93b-3805-4903-becf-43b2fc78bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Ensure 'In_Scope' column is boolean\n",
    "index_df['In_Scope'] = index_df['In_Scope'].astype(bool)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Extract list of in-scope vCenter instances\n",
    "inscope_vcenter_instances = inscope_df['vCenter'].tolist()\n",
    "\n",
    "# Function to read RVTools Excel files while excluding 'index_template.xlsx' and 'index.xlsx'\n",
    "def read_rvtools_excel_files(directory, vcenters):\n",
    "    rvtools_data = {}\n",
    "    exclude_files = {\"index_template.xlsx\", \"index.xlsx\"}  # Set of filenames to exclude\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xlsx\") and file.lower() not in exclude_files:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                xls = pd.ExcelFile(file_path)\n",
    "                sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "                rvtools_data[file] = sheets\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    return rvtools_data\n",
    "\n",
    "# Read the RVTools Excel files\n",
    "rvtools_data = read_rvtools_excel_files(\"../data\", inscope_vcenter_instances)\n",
    "\n",
    "# Display the loaded data (for demonstration purposes)\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    print(f'Processed RVTools File: {filename}')\n",
    "    for sheet_name, df in sheets.items():\n",
    "        print(f\"  Sheet: {sheet_name} Shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖÔ∏è \"f'Total files processed: {len(rvtools_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Create the consolidated data frames\n",
    "\n",
    "Create two consolidated dataframes containing information from all `vCenter` instances:\n",
    "\n",
    "1. One dataframe containing all the `vInfo` (VM's) details\n",
    "2. The second one containing all the `vHost` (HW) details\n",
    "\n",
    "These will be used later to summarize the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize the lists\n",
    "vinfo_sheets = []\n",
    "vhost_sheets = []\n",
    "\n",
    "# Load vInfo and vHost sheets into the lists\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    vCenter = filename.split('.')[0].lower()  # Extract vCenter instance name from the filename\n",
    "\n",
    "    # Ensure 'vInfo' and 'vHost' sheets exist before processing\n",
    "    if 'vInfo' in sheets and 'vHost' in sheets:\n",
    "        # Add the vCenter instance name to each sheet\n",
    "        sheets['vInfo']['vCenter'] = vCenter\n",
    "        sheets['vHost']['vCenter'] = vCenter\n",
    "\n",
    "        # Append to the respective lists\n",
    "        vinfo_sheets.append(sheets['vInfo'])\n",
    "        vhost_sheets.append(sheets['vHost'])\n",
    "\n",
    "# Filter out empty or all-NA DataFrames\n",
    "vinfo_sheets = [df for df in vinfo_sheets if not df.dropna(how='all').empty]\n",
    "vhost_sheets = [df for df in vhost_sheets if not df.dropna(how='all').empty]\n",
    "\n",
    "# Concatenate the filtered DataFrames only if there are valid sheets\n",
    "consolidated_vinfo_df = pd.concat(vinfo_sheets, ignore_index=True) if vinfo_sheets else pd.DataFrame()\n",
    "consolidated_vhost_df = pd.concat(vhost_sheets, ignore_index=True) if vhost_sheets else pd.DataFrame()\n",
    "\n",
    "# Verify data ingestion success with improved readability\n",
    "print(\"üîç Data Ingestion Verification\")\n",
    "print(\"\\n‚úÖÔ∏è \"f\"Total vInfo (Total VM's) records: {len(consolidated_vinfo_df)}\")\n",
    "print(\"‚úÖÔ∏è \"f\"Total vHost (Total HW) records: {len(consolidated_vhost_df)}\\n\")\n",
    "\n",
    "if not consolidated_vinfo_df.empty:\n",
    "    print(\"üìÑ Sample vInfo Data:\")\n",
    "    print(consolidated_vinfo_df.head(2).to_string(index=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe759-4222-4227-a0e1-896b7e60de1b",
   "metadata": {},
   "source": [
    "### Distribution of VMs per In-Scope vCenter\n",
    "\n",
    "What is the total percentage of VM's per vCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure that consolidated_vinfo_df exists before proceeding\n",
    "try:\n",
    "    # Group by vCenter and count VMs\n",
    "    grouped_summary = consolidated_vinfo_df.groupby(\"vCenter\").size().reset_index(name=\"Count\")\n",
    "\n",
    "    # Create a pivot table for better visualization\n",
    "    grouped_summary_pivot = grouped_summary.pivot_table(values=\"Count\", index=\"vCenter\", aggfunc=\"sum\")\n",
    "\n",
    "    # Print summary\n",
    "    total_vms = grouped_summary[\"Count\"].sum()\n",
    "    total_vcenters = grouped_summary.shape[0]\n",
    "\n",
    "    print(\"üîç \"f\"Overall Distribution of {total_vms:,} VMs in the {total_vcenters:,} vCenter instances:\")\n",
    "    print(grouped_summary_pivot)\n",
    "    print(\"\\nüìù This is the TOTAL VM count, and WILL include VM templates, SRM Placeholders, Powered Off & Orphaned Objects...\")\n",
    "\n",
    "    # Generate explode values to emphasize each slice\n",
    "    explode_values = [0.05] * len(grouped_summary)  # Adjust explosion for all slices\n",
    "\n",
    "    # Get colors from the tab20 colormap\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    colors = [cmap(i % 10) for i in range(len(grouped_summary))]  # Cycle through the colormap\n",
    "\n",
    "    # Plot an exploded pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(\n",
    "        grouped_summary[\"Count\"],\n",
    "        labels=grouped_summary[\"vCenter\"],\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=140,\n",
    "        explode=explode_values,  # Add explode effect\n",
    "        shadow=True,  # Add shadow for better visualization\n",
    "        colors=colors  # Use tab20 colors\n",
    "    )\n",
    "    plt.title(\"\\nDistribution of VMs by vCenter Instances\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError as e:\n",
    "    print(\"Error: The dataset 'consolidated_vinfo_df' is not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Clean the VM List\n",
    "Filter the consolidated `vInfo` content using the following criteria:\n",
    "\n",
    "1. Remove all `template` entries\n",
    "2. Remove all `SRM Placeholder` entries\n",
    "3. Remove all `orphaned VM object` entries\n",
    "4. Remove all `other objects`\n",
    "\n",
    "Adjust `ignored_patterns` as well as `os_filter_patterns` to suit your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61863a1-bd86-445c-b261-d22d12c13cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Define patterns to ignore in the VM column\n",
    "ignore_patterns = [\"virtual_appliance\", \"virtual appliance\", \"CTX\"]\n",
    "\n",
    "# Define OS types to filter out (only using \"OS according to the VMware Tools\")\n",
    "os_filter_patterns = [\n",
    "    r\"^Microsoft Windows 7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003 Standard(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other$\",\n",
    "    r\"^Other\\s+\\d+\\.\\d+.*$\",\n",
    "    r\"^Other 3\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 3\\.x Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 4\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 5\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^VMware Photon OS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008 R2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Enterprise Linux (4|5|6)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Debian GNU/Linux (7|8|9|10|11)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS (4|5|6|7)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6/7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5 or later(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS Stream 8(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Ubuntu Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Linux \\d+\\.\\d+.*$\",\n",
    "    r\"^Appgate(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Amazon Linux 2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other Linux(?:\\s+.*)?$\",\n",
    "    r\"^FreeBSD(?:\\s+.*)?$\",\n",
    "    r\"^RiOS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Fedora(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^SuSE Linux Enterprise (11|12)(?:\\s*\\(.*\\))?$\"\n",
    "]\n",
    "\n",
    "def clean_os_name(os_name):\n",
    "    \"\"\"Normalize OS names by removing extra spaces and redundant information.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return ''\n",
    "    os_name = os_name.strip()\n",
    "    os_name = re.sub(r\"\\s*\\(.*\\)$\", \"\", os_name)  # Remove (32-bit) / (64-bit)\n",
    "    os_name = re.sub(r\"\\s+\", \" \", os_name)  # Remove extra spaces\n",
    "    return os_name\n",
    "\n",
    "def os_filter(os_name):\n",
    "    \"\"\"Check if the OS should be filtered based on the defined patterns.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return False\n",
    "    os_name_clean = clean_os_name(os_name)\n",
    "    return any(re.fullmatch(pattern, os_name_clean, re.IGNORECASE) for pattern in os_filter_patterns)\n",
    "\n",
    "# Load or define consolidated_vinfo_df before processing\n",
    "if 'consolidated_vinfo_df' not in globals():\n",
    "    raise ValueError(\"consolidated_vinfo_df is not defined.\")\n",
    "\n",
    "# Ensure OS column is populated\n",
    "consolidated_vinfo_df['OS Effective'] = consolidated_vinfo_df['OS according to the VMware Tools'].fillna(\n",
    "    consolidated_vinfo_df['OS according to the configuration file'])\n",
    "\n",
    "# Normalize OS names before filtering\n",
    "consolidated_vinfo_df['Cleaned OS'] = consolidated_vinfo_df['OS Effective'].apply(clean_os_name)\n",
    "\n",
    "# Apply filtering logic\n",
    "consolidated_vinfo_df['Exclusion Reason'] = ''\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['VM'].str.contains('|'.join(ignore_patterns), case=False, na=False), 'Exclusion Reason'] = 'Ignored VM Pattern'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Cleaned OS'].apply(os_filter), 'Exclusion Reason'] = 'Excluded OS'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Template'].fillna(False) == True, 'Exclusion Reason'] = 'Template'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['SRM Placeholder'].fillna(False) == True, 'Exclusion Reason'] = 'SRM Placeholder'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Connection state'].fillna('').str.lower() == 'orphaned', 'Exclusion Reason'] = 'Orphaned VM'\n",
    "\n",
    "# Only exclude powered-off VMs if their OS is in the exclusion list\n",
    "consolidated_vinfo_df.loc[(consolidated_vinfo_df['Powerstate'].fillna('').str.lower() == 'poweredoff') & \n",
    "                          (consolidated_vinfo_df['Cleaned OS'].apply(os_filter)), 'Exclusion Reason'] = 'Powered Off'\n",
    "\n",
    "# Separate out categories\n",
    "ignored_vm_artifacts = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == 'Excluded OS']\n",
    "ignored_artifacts = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'].isin(['Template', 'SRM Placeholder', 'Orphaned VM'])]\n",
    "\n",
    "# Filter only in-scope VMs\n",
    "filtered_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Count the number of VMs in each category\n",
    "ignored_vm_artifacts_count = len(ignored_vm_artifacts)\n",
    "ignored_artifacts_count = len(ignored_artifacts)\n",
    "filtered_vm_count = len(filtered_vinfo_df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüîç Filtering Summary\")\n",
    "print(f\"üî∂ Out-of-Scope VM count: {ignored_vm_artifacts_count:,} (Removed due to OS filtering).\")\n",
    "print(f\"üî∂ Ignored Artifacts: {ignored_artifacts_count:,} (Templates, SRM Placeholders, Orphaned).\")\n",
    "print(f\"üî∑ In-Scope VM count: {filtered_vm_count:,}.\\n\")\n",
    "\n",
    "# Display the list of \"in-scope\" VMs after filtering\n",
    "if not filtered_vinfo_df.empty:\n",
    "    print(\"\\n‚úÖ Sample of filtered (In-Scope) VMs:\")\n",
    "    display(filtered_vinfo_df[['VM', 'OS Effective']].head())\n",
    "\n",
    "# Display the list of \"ignored\" VMs\n",
    "if not ignored_vm_artifacts.empty:\n",
    "    print(\"\\n‚ùå Sample of filtered (Out-of-Scope) VMs:\")\n",
    "    display(ignored_vm_artifacts[['VM', 'OS Effective', 'Exclusion Reason']].head())\n",
    "\n",
    "if not ignored_artifacts.empty:\n",
    "    print(\"\\n‚ùå Sample of filtered (Out-of-Scope) Artifacts [Templates, SRM Placeholders & other Orphaned objects]:\")\n",
    "    display(ignored_artifacts[['VM', 'OS Effective', 'Exclusion Reason']].head())\n",
    "\n",
    "# Create pie chart\n",
    "labels = ['Out-of-Scope VMs', 'Ignored Artifacts', 'In-Scope VMs']\n",
    "sizes = [ignored_vm_artifacts_count, ignored_artifacts_count, filtered_vm_count]\n",
    "colors = ['lightcoral', 'lightskyblue', 'lightgreen']\n",
    "explode = (0.1, 0.1, 0)  # explode the 'Excluded OS' and 'Ignored Artifacts' sections for emphasis\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, explode=explode, shadow=True, startangle=140)\n",
    "plt.title(\"\\nVM Filtering Breakdown\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Summary of vInfo & vHosts after cleanup\n",
    "\n",
    "Now we have clean data to play with going forward..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select the in-scope vCenter instances\n",
    "inscope_vinfo_condition = filtered_vinfo_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vinfo_df = filtered_vinfo_df[inscope_vinfo_condition]\n",
    "\n",
    "# Summary stats for in-scope VMs\n",
    "inscope_vm_count = len(filtered_vinfo_df)\n",
    "percent_inscope_vms = (inscope_vm_count / len(filtered_vinfo_df)) * 100.0\n",
    "\n",
    "print(\"\\nüîç VM (vInfo) Summary\")\n",
    "print(f\"‚úÖ {inscope_vm_count:,} VMs are in-scope ({percent_inscope_vms:0.2f}% of total VMs).\\n\")\n",
    "\n",
    "# Create a pivot table for VMs by vCenter\n",
    "vm_pivot = inscope_vinfo_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "vm_pivot.columns = [\"vCenter\", \"VM Count\"]\n",
    "display(vm_pivot)\n",
    "\n",
    "# Select the in-scope hosts\n",
    "inscope_vhost_condition = consolidated_vhost_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vhost_df = consolidated_vhost_df[inscope_vhost_condition]\n",
    "\n",
    "# Summary stats for in-scope hosts\n",
    "inscope_host_count = len(inscope_vhost_df)\n",
    "percent_inscope_hosts = (inscope_host_count / len(consolidated_vhost_df)) * 100.0\n",
    "\n",
    "print(\"\\nüîç Host (vHost) Summary\")\n",
    "print(f\"‚úÖ {inscope_host_count:,} hosts are in-scope ({percent_inscope_hosts:0.2f}% of total hosts).\\n\")\n",
    "\n",
    "# Create a pivot table for hosts by vCenter\n",
    "host_pivot = inscope_vhost_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "host_pivot.columns = [\"vCenter\", \"Host Count\"]\n",
    "display(host_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb6031-4de5-4f22-937a-ece58b28b701",
   "metadata": {},
   "source": [
    "# ========== Analysis =========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The primary analysis begins from this section forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1- Create a consolidated view of the client's landscape\n",
    "\n",
    "Summary of VM's (vCPU, Memory, etc) & Host's (CPU, Cores, etc)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === vInfo Pivot Table (VM-Level Info) ===\n",
    "vinfo_pivot_df = filtered_vinfo_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['VM', 'CPUs', 'Memory', 'NICs', 'Provisioned MiB'],\n",
    "    aggfunc={\n",
    "        'VM': 'count',\n",
    "        'CPUs': 'sum',\n",
    "        'Memory': 'sum',\n",
    "        'NICs': 'sum',\n",
    "        'Provisioned MiB': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# === vHost Pivot Table (Host-Level Info) ===\n",
    "vhost_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['Host', '# VMs total', '# CPU', '# Cores'],\n",
    "    aggfunc={\n",
    "        'Host': 'count',\n",
    "        '# CPU': 'sum',\n",
    "        '# Cores': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# === Convert Memory to GB/TB ===\n",
    "def format_storage(mib_value):\n",
    "    if mib_value >= 1_000_000:  # Convert to TB if ‚â• 1,000,000 MiB\n",
    "        return f\"{mib_value / 1_048_576:.2f} TB\"\n",
    "    return f\"{mib_value / 1024:.2f} GB\"  # Convert to GB otherwise\n",
    "\n",
    "# Apply formatting for Memory and create a new Total Disk Capacity column\n",
    "vinfo_pivot_df['Total Disk Capacity'] = vinfo_pivot_df['Provisioned MiB'].apply(format_storage)\n",
    "vinfo_pivot_df['Memory'] = vinfo_pivot_df['Memory'].apply(format_storage)\n",
    "\n",
    "# Drop 'Provisioned MiB' from the final display\n",
    "vinfo_pivot_df = vinfo_pivot_df.drop(columns=['Provisioned MiB'])\n",
    "\n",
    "# === Pretty Display for Each Table ===\n",
    "try:\n",
    "    from IPython.display import display  # Works for Jupyter Notebook\n",
    "\n",
    "    print(\"\\nüîç VM Info (vInfo)\")\n",
    "    display(vinfo_pivot_df)\n",
    "\n",
    "    print(\"\\nüîç Host Info (vHost)\")\n",
    "    display(vhost_pivot_df)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nüîç VM Info (vInfo)\")\n",
    "    print(vinfo_pivot_df)\n",
    "\n",
    "    print(\"\\nüîç Host Info (vHost)\")\n",
    "    print(vhost_pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 2- Summarize the Operating Systems\n",
    "\n",
    "In this section, we summarize the guest operating systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Define patterns to ignore in the VM column\n",
    "ignore_patterns = [\"virtual_appliance\", \"virtual appliance\", \"CTX\"]\n",
    "\n",
    "# Define OS types to filter out (only using \"OS according to the VMware Tools\")\n",
    "os_filter_patterns = [\n",
    "    r\"^Microsoft Windows 7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003 Standard(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other$\",\n",
    "    r\"^Other\\s+\\d+\\.\\d+.*$\",\n",
    "    r\"^Other 3\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 3\\.x Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 4\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 5\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^VMware Photon OS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008 R2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Enterprise Linux (4|5|6)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Debian GNU/Linux (7|8|9|10|11)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS (4|5|6|7)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6/7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5 or later(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS Stream 8(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Ubuntu Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Linux \\d+\\.\\d+.*$\",\n",
    "    r\"^Appgate(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Amazon Linux 2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other Linux(?:\\s+.*)?$\",\n",
    "    r\"^FreeBSD(?:\\s+.*)?$\",\n",
    "    r\"^RiOS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Fedora(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^SuSE Linux Enterprise (11|12)(?:\\s*\\(.*\\))?$\"\n",
    "]\n",
    "\n",
    "def clean_os_name(os_name):\n",
    "    \"\"\"Normalize OS names by removing extra spaces and redundant information.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return ''\n",
    "    os_name = os_name.strip()\n",
    "    os_name = re.sub(r\"\\s*\\(.*\\)$\", \"\", os_name)  # Remove (32-bit) / (64-bit)\n",
    "    os_name = re.sub(r\"\\s+\", \" \", os_name)  # Remove extra spaces\n",
    "    return os_name\n",
    "\n",
    "def os_filter(os_name):\n",
    "    \"\"\"Check if the OS should be filtered based on the defined patterns.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return False\n",
    "    os_name_clean = clean_os_name(os_name)\n",
    "    return any(re.fullmatch(pattern, os_name_clean, re.IGNORECASE) for pattern in os_filter_patterns)\n",
    "\n",
    "# Load or define consolidated_vinfo_df before processing\n",
    "if 'consolidated_vinfo_df' not in globals():\n",
    "    raise ValueError(\"consolidated_vinfo_df is not defined.\")\n",
    "\n",
    "# Ensure OS column is populated\n",
    "consolidated_vinfo_df['OS Effective'] = consolidated_vinfo_df['OS according to the VMware Tools'].fillna(\n",
    "    consolidated_vinfo_df['OS according to the configuration file'])\n",
    "\n",
    "# Normalize OS names before filtering\n",
    "consolidated_vinfo_df['Cleaned OS'] = consolidated_vinfo_df['OS Effective'].apply(clean_os_name)\n",
    "\n",
    "# Apply filtering logic\n",
    "consolidated_vinfo_df['Exclusion Reason'] = ''\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['VM'].str.contains('|'.join(ignore_patterns), case=False, na=False), 'Exclusion Reason'] = 'Ignored VM Pattern'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Cleaned OS'].apply(os_filter), 'Exclusion Reason'] = 'Excluded OS'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Template'].fillna(False) == True, 'Exclusion Reason'] = 'Template'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['SRM Placeholder'].fillna(False) == True, 'Exclusion Reason'] = 'SRM Placeholder'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Connection state'].fillna('').str.lower() == 'orphaned', 'Exclusion Reason'] = 'Orphaned VM'\n",
    "\n",
    "# Only exclude powered-off VMs if their OS is in the exclusion list\n",
    "consolidated_vinfo_df.loc[(consolidated_vinfo_df['Powerstate'].fillna('').str.lower() == 'poweredoff') & \n",
    "                          (consolidated_vinfo_df['Cleaned OS'].apply(os_filter)), 'Exclusion Reason'] = 'Powered Off'\n",
    "\n",
    "# Filter only in-scope VMs\n",
    "filtered_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Create the pivot table\n",
    "inscope_guest_os_pivot = filtered_vinfo_df.pivot_table(index='Cleaned OS', values='VM', aggfunc='count')\n",
    "inscope_guest_os_pivot = inscope_guest_os_pivot.sort_values(by='VM', ascending=True)\n",
    "\n",
    "# Check if pivot table is empty\n",
    "if inscope_guest_os_pivot.empty:\n",
    "    raise ValueError(\"The pivot table is empty after filtering. Check input data.\")\n",
    "\n",
    "# Calculate percentages\n",
    "total_vms = inscope_guest_os_pivot['VM'].sum()\n",
    "percentages = (inscope_guest_os_pivot['VM'] / total_vms * 100).round(1)\n",
    "\n",
    "# Create figure and axis with better layout management\n",
    "fig, ax = plt.subplots(figsize=(14, max(6, len(inscope_guest_os_pivot) * 0.4)), constrained_layout=True)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(inscope_guest_os_pivot.index, inscope_guest_os_pivot['VM'], color=plt.cm.tab20.colors)\n",
    "\n",
    "# Add labels to bars\n",
    "for bar, count, percentage in zip(bars, inscope_guest_os_pivot['VM'], percentages):\n",
    "    ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "            f\"{count} VMs ({percentage}%)\", va='center', fontsize=8, color='black')\n",
    "\n",
    "# Style and labels\n",
    "ax.set_xlabel(\"Number of VMs\", fontsize=10)\n",
    "ax.set_ylabel(\"Operating Systems\", fontsize=10)\n",
    "ax.set_title(\"In-Scope OS Distribution\", fontsize=12, pad=20)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Explicitly set y-ticks before modifying labels to avoid warnings\n",
    "ax.set_yticks(range(len(inscope_guest_os_pivot)))\n",
    "ax.set_yticklabels(inscope_guest_os_pivot.index, fontsize=9, rotation=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3- OS's with over 500 VMs Associated\n",
    "\n",
    "In this section, we summarize the guest operating systems that have more than 500 VM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter and display OSs with over 500 VMs\n",
    "over_500_vms = inscope_guest_os_pivot[inscope_guest_os_pivot['VM'] > 500]\n",
    "display(over_500_vms)\n",
    "\n",
    "# Check if there are any OSs with more than 500 VMs\n",
    "if over_500_vms.empty:\n",
    "    print(\"‚ö†Ô∏è No operating systems have more than 500 VMs.\")\n",
    "else:\n",
    "    # Pie chart for OSs with over 500 VMs\n",
    "    plt.figure(figsize=(8, 8))  # Slightly wider for better readability\n",
    "\n",
    "    # Define a color palette\n",
    "    colors = plt.cm.tab20.colors[:len(over_500_vms)]\n",
    "\n",
    "    # Define explode values (proportionally exploding each slice)\n",
    "    explode = [0.05] * len(over_500_vms)  # Exploding all slices for visibility\n",
    "\n",
    "    # Generate exploded pie chart with shadow effect\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        over_500_vms['VM'],\n",
    "        labels=over_500_vms.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "        explode=explode,  # Exploding slices\n",
    "        shadow=True,  # Adding shadow effect\n",
    "    )\n",
    "\n",
    "    # Improve text readability\n",
    "    for text in texts:\n",
    "        text.set_fontsize(10)  # Adjust label size\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(10)  # Adjust percentage text size\n",
    "        autotext.set_color('white')  # Improve visibility\n",
    "\n",
    "    # Add title\n",
    "    plt.title('In-Scope OS\\'s with Over 500 VMs', fontsize=12, pad=20)\n",
    "\n",
    "    # Adjust layout to prevent labels from overlapping\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0133b5-3674-4a1f-9034-6f7433e70acc",
   "metadata": {},
   "source": [
    "### 4- Group VMs by Memory Size\n",
    "\n",
    "In this section, we group the VMs by their memory-size tiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define memory tiers (in GB) in ascending order\n",
    "memory_tiers = {\n",
    "    '0-4 GB': (0, 4),\n",
    "    '4-16 GB': (4, 16),\n",
    "    '16-32 GB': (16, 32),\n",
    "    '32-64 GB': (32, 64),\n",
    "    '64-128 GB': (64, 128),\n",
    "    '128-256 GB': (128, 256),\n",
    "    '256+ GB': (256, float('inf'))\n",
    "}\n",
    "\n",
    "# Function to categorize memory\n",
    "def categorize_memory(memory_gb):\n",
    "    for tier, (lower, upper) in memory_tiers.items():\n",
    "        if lower <= memory_gb < upper:\n",
    "            return tier\n",
    "    return 'Unknown'\n",
    "\n",
    "# Ensure 'filtered_vinfo_df' is defined before proceeding\n",
    "if 'filtered_vinfo_df' not in locals():\n",
    "    raise ValueError(\"Error: 'filtered_vinfo_df' is not defined. Ensure the DataFrame exists before running this script.\")\n",
    "\n",
    "# Filter only In-Scope OS VMs\n",
    "in_scope_vms = filtered_vinfo_df[filtered_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Ensure 'Memory' column is in MiB, then convert to GB\n",
    "in_scope_vms = in_scope_vms.copy()  # Explicit copy to avoid SettingWithCopyWarning\n",
    "in_scope_vms.loc[:, 'Memory GB'] = in_scope_vms['Memory'] / 1024\n",
    "\n",
    "# Apply categorization\n",
    "in_scope_vms.loc[:, 'Memory Tier'] = in_scope_vms['Memory GB'].apply(categorize_memory)\n",
    "\n",
    "# Create a pivot table summarizing VM counts per memory tier\n",
    "memory_tier_summary = (\n",
    "    in_scope_vms.groupby('Memory Tier')\n",
    "    .agg({'VM': 'count'})\n",
    "    .rename(columns={'VM': 'VM Count'})\n",
    ")\n",
    "\n",
    "# Sort the memory tiers in ascending order based on the predefined tier order\n",
    "memory_tier_summary = memory_tier_summary.reindex(memory_tiers.keys()).fillna(0)\n",
    "\n",
    "# Display the results\n",
    "try:\n",
    "    from IPython.display import display  # Works for Jupyter Notebook\n",
    "    print(\"\\nüîç Memory Tier Summary (In-Scope OS Only)\")\n",
    "    display(memory_tier_summary)\n",
    "except ImportError:\n",
    "    print(\"\\nüîç Memory Tier Summary (In-Scope OS Only)\")\n",
    "    print(memory_tier_summary)\n",
    "\n",
    "# Generate pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "explode_values = [0.05] * len(memory_tier_summary)  # Exploding all slices slightly\n",
    "colors = plt.cm.tab20.colors[:len(memory_tier_summary)]  # Use tab20 colors\n",
    "\n",
    "plt.pie(\n",
    "    memory_tier_summary['VM Count'],\n",
    "    labels=memory_tier_summary.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    explode=explode_values,\n",
    "    shadow=True,\n",
    "    colors=colors\n",
    ")\n",
    "plt.title(\"VM Distribution by Memory Size Tier (In-Scope OS Only)\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5- Group VMs by Disk Size\n",
    "\n",
    "This section groups VMs into categories defined by allocated disk-size tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e858812-f4c0-4853-8c8e-70f7542e136b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create explicit copy to avoid SettingWithCopyWarning\n",
    "filtered_vinfo_df = filtered_vinfo_df.copy()\n",
    "\n",
    "# Ensure only VMs from In-Scope OS are included\n",
    "filtered_vinfo_df = filtered_vinfo_df[filtered_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Convert disk size to TB and categorize into tiers\n",
    "mib_to_tb_conversion_factor = 2**20 / 10**12\n",
    "filtered_vinfo_df.loc[:, 'Disk Size TB'] = filtered_vinfo_df['Provisioned MiB'] * mib_to_tb_conversion_factor\n",
    "\n",
    "# Identify VMs with missing or zero disk size\n",
    "no_disk_size_vms_df = filtered_vinfo_df[(filtered_vinfo_df['Disk Size TB'].isna()) | (filtered_vinfo_df['Disk Size TB'] == 0)]\n",
    "no_disk_size_vm_count = no_disk_size_vms_df.shape[0]\n",
    "\n",
    "# Filter out VMs without disk size for further analysis\n",
    "filtered_disk_df = filtered_vinfo_df[filtered_vinfo_df['Disk Size TB'] > 0].copy()\n",
    "\n",
    "# Define disk size bins and labels\n",
    "disk_size_bins = [0, 1, 2, 10, 20, 50, 100, float('inf')]\n",
    "disk_bin_labels = ['Tiny (<1 TB)', 'Easy (<=2 TB)', 'Medium (<=10 TB)', 'Hard (<=20 TB)', \n",
    "                   'Very Hard (<=50 TB)', 'White Glove (<=100 TB)', 'Extreme (>100 TB)']\n",
    "\n",
    "filtered_disk_df.loc[:, 'Disk Size Tiers'] = pd.cut(\n",
    "    filtered_disk_df['Disk Size TB'],\n",
    "    bins=disk_size_bins,\n",
    "    labels=disk_bin_labels\n",
    ").astype(str)\n",
    "\n",
    "# Pivot table based on disk size tiers\n",
    "disk_tier_pivot_df = filtered_disk_df.pivot_table(\n",
    "    index='Disk Size Tiers', \n",
    "    values=['VM', 'Disk Size TB'], \n",
    "    aggfunc={'VM': 'count', 'Disk Size TB': 'sum'},\n",
    "    observed=False\n",
    ")\n",
    "\n",
    "# Sort index to maintain ascending order\n",
    "disk_tier_pivot_df = disk_tier_pivot_df.reindex(disk_bin_labels)\n",
    "\n",
    "# Fill NaN values to avoid errors\n",
    "disk_tier_pivot_df = disk_tier_pivot_df.fillna(0)\n",
    "\n",
    "# Add total row with rounded 'Disk Size TB' to nearest 0.5 TB\n",
    "total_row = pd.DataFrame({\n",
    "    'VM': [disk_tier_pivot_df['VM'].sum()],\n",
    "    'Disk Size TB': [round(disk_tier_pivot_df['Disk Size TB'].sum() * 2) / 2]\n",
    "}, index=['Total'])\n",
    "\n",
    "# Combine pivot table with total row\n",
    "disk_tier_pivot_with_total = pd.concat([disk_tier_pivot_df, total_row])\n",
    "\n",
    "# Fill NaN values and format for display\n",
    "formatted_table = disk_tier_pivot_with_total.copy()\n",
    "formatted_table['VM'] = formatted_table['VM'].fillna(0).astype(int)\n",
    "formatted_table['Disk Size TB'] = formatted_table['Disk Size TB'].fillna(0).apply(lambda x: f\"{x:,.2f}\")\n",
    "formatted_table['VM'] = formatted_table['VM'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Display table\n",
    "print(\"\\U0001F50D Tier Summary with Total Disk (Formatted):\")\n",
    "print(formatted_table.to_string())\n",
    "\n",
    "# Calculate percentages for VM distribution in each disk tier\n",
    "vm_counts = disk_tier_pivot_df['VM'].sum()\n",
    "percentages = (disk_tier_pivot_df['VM'] / vm_counts) * 100\n",
    "\n",
    "# Generate labels\n",
    "labels = [f\"{tier} - {pct:.1f}% ({count} VMs)\" \n",
    "          for tier, pct, count in zip(disk_tier_pivot_df.index, percentages, disk_tier_pivot_df['VM'])]\n",
    "\n",
    "# Automatically assign colors based on number of categories\n",
    "num_tiers = len(disk_bin_labels)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, num_tiers))\n",
    "\n",
    "# Explode effect to emphasize slices\n",
    "explode = [0.05] * num_tiers  # Adjust explosion for each tier\n",
    "\n",
    "# Pie Chart: Automatically assigned colors with explode effect\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.pie(disk_tier_pivot_df['VM'], colors=colors, autopct='%1.1f%%', startangle=140, explode=explode)\n",
    "plt.title('VM Distribution by Tier (Tiny, Easy, Medium, etc.)', fontsize=12)\n",
    "plt.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for VMs without disk size\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(['No Disk Size'], [no_disk_size_vm_count], color='purple')\n",
    "plt.title('VMs without Disk Size Information', fontsize=12)\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 6- Categorize Host Compute Nodes\n",
    "\n",
    "Categorize the compute nodes for all vCenters based on their model number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Group all hosts by model\n",
    "all_host_model_pivot_df = consolidated_vhost_df.pivot_table(index=['Vendor', 'Model'], values='Host', aggfunc='count')\n",
    "display(all_host_model_pivot_df)\n",
    "\n",
    "# Bar chart for all host models\n",
    "plt.figure(figsize=(12, 8))  # Make it wider to avoid label overlap\n",
    "\n",
    "# Generate a colormap with enough distinct colors\n",
    "norm = mcolors.Normalize(vmin=0, vmax=len(all_host_model_pivot_df)-1)\n",
    "colors = cm.tab20(norm(range(len(all_host_model_pivot_df))))\n",
    "\n",
    "# Create a bar chart with automatic colors\n",
    "bar_plot = all_host_model_pivot_df['Host'].plot(kind='bar', color=colors, edgecolor='black', figsize=(12, 8))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Host Node Models', fontsize=12)\n",
    "plt.xlabel('Host Model', fontsize=10)\n",
    "plt.ylabel('Host Count', fontsize=10)\n",
    "\n",
    "# Improve x-tick readability by rotating the labels\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "\n",
    "# Add totals on top of each bar\n",
    "for bar in bar_plot.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{int(bar.get_height())}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Show the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 7- Categorize Host Compute Nodes by vCenter\n",
    "\n",
    "Categorize compute nodes by their model number for each vCenter separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Group all the hosts by their model and vCenter\n",
    "all_host_model_vcenter_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index=['vCenter', 'Vendor', 'Model'],\n",
    "    values=['Host'],\n",
    "    aggfunc={'Host': 'count'},\n",
    "    observed=False,\n",
    "    margins=False,\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "print(\"üîç \"f'Distribution of ALL host models by vCenter:\\n')\n",
    "print(all_host_model_vcenter_pivot_df)\n",
    "all_host_model_vcenter_pivot_df.to_clipboard(excel=True)\n",
    "\n",
    "# Bar chart creation based on the pivot table (sorted by host count)\n",
    "plt.figure(figsize=(14, 8))  # Increase figure size for better visibility\n",
    "\n",
    "# Prepare labels and data\n",
    "labels = all_host_model_vcenter_pivot_df.index.map(lambda x: f'{x[1]} {x[2]} ({x[0]})')  # Combine Vendor, Model, and vCenter in the label\n",
    "sizes = all_host_model_vcenter_pivot_df['Host']\n",
    "\n",
    "# Automatically assign colors using the 'tab20' colormap\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(sizes)))\n",
    "\n",
    "# Create the vertical bar chart\n",
    "bars = plt.bar(labels, sizes, color=colors, edgecolor='black')\n",
    "\n",
    "# Add text annotations (totals) on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2, \n",
    "        yval, \n",
    "        int(yval), \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "# Add title and formatting\n",
    "plt.title('Host Node Models', fontsize=14)\n",
    "plt.xlabel('Host Model', fontsize=12)\n",
    "plt.ylabel('Number of Hosts', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 8- Count the ESXi Datacenters & Clusters\n",
    "\n",
    "Summary for the total ESXi datacenters & clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove NaN values from Datacenter and Cluster\n",
    "filtered_vinfo_df = filtered_vinfo_df.dropna(subset=['Datacenter', 'Cluster'])\n",
    "\n",
    "# Get unique Datacenters\n",
    "datacenters = filtered_vinfo_df['Datacenter'].unique()\n",
    "\n",
    "# Print total number of in-scope VMware Datacenters\n",
    "print(\"üîç \"f'Total VMware Datacenters: {len(datacenters):,}')\n",
    "\n",
    "# Get unique Clusters\n",
    "clusters = filtered_vinfo_df['Cluster'].unique()\n",
    "\n",
    "# Print total number of in-scope ESXi clusters\n",
    "print(\"üîç \"f'Total ESXi clusters: {len(clusters):,}')\n",
    "\n",
    "# Pivot the Cluster information on the Datacenters\n",
    "inscope_datacenter_pivot = filtered_vinfo_df.pivot_table(index='Datacenter',\n",
    "                                                          values='Cluster',\n",
    "                                                          aggfunc=pd.Series.nunique)  # Count unique clusters per datacenter\n",
    "\n",
    "# Rename column for clarity\n",
    "inscope_datacenter_pivot.rename(columns={'Cluster': 'Cluster_Count'}, inplace=True)\n",
    "\n",
    "# Sort by Cluster_Count in descending order\n",
    "inscope_datacenter_pivot = inscope_datacenter_pivot.sort_values(by='Cluster_Count', ascending=False)\n",
    "\n",
    "# Calculate the total number of clusters from the pivot table\n",
    "total_clusters_from_pivot = int(inscope_datacenter_pivot['Cluster_Count'].sum())  # Convert to Python int\n",
    "\n",
    "# Calculate the total number of datacenters\n",
    "total_datacenters = int(inscope_datacenter_pivot.shape[0])  # Convert to Python int\n",
    "\n",
    "# Show only the top 10 datacenters in the summary\n",
    "print(f'\\nüîç Distribution of Clusters to Datacenters (Top 10 by Cluster Count):')\n",
    "print(inscope_datacenter_pivot.head(10).to_string(index=True))  # Shows only top 10 sorted\n",
    "print(f'\\nüî• Total Clusters across all Datacenters: {total_clusters_from_pivot}')\n",
    "print(f'üî• Total number of Datacenters: {total_datacenters}')\n",
    "\n",
    "# Debug: Compare Total ESXi clusters vs. Sum of clusters per datacenter\n",
    "if len(clusters) != total_clusters_from_pivot:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: The total ESXi clusters count does not match the sum of clusters across datacenters.\")\n",
    "    print(f\"‚ÄºÔ∏è Total ESXi clusters (unique across dataset): {len(clusters):,}\")\n",
    "    print(f\"‚ÄºÔ∏è Total Clusters from pivot table (sum per datacenter): {total_clusters_from_pivot:,}\")\n",
    "\n",
    "    # Find clusters mapped to multiple datacenters\n",
    "    cluster_datacenter_mapping = filtered_vinfo_df.groupby('Cluster')['Datacenter'].nunique()\n",
    "    multi_dc_clusters = cluster_datacenter_mapping[cluster_datacenter_mapping > 1]\n",
    "\n",
    "    if not multi_dc_clusters.empty:\n",
    "        print(\"\\nüîç Clusters that appear in multiple datacenters (Top 10 shown):\")\n",
    "        print(multi_dc_clusters.head(10).to_string(index=True))  # Shows only top 10\n",
    "\n",
    "# Copy full results to clipboard for easy pasting into Excel\n",
    "inscope_datacenter_pivot.to_clipboard(excel=True)\n",
    "\n",
    "# Create a pie chart (using only the top 10 datacenters for clarity)\n",
    "top_10_pivot = inscope_datacenter_pivot.head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_pivot['Cluster_Count'], labels=top_10_pivot.index, \n",
    "        autopct='%1.1f%%', startangle=140, explode=[0.05] * len(top_10_pivot))\n",
    "plt.title('\\n Clusters distributed to Top 10 Datacenters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 9- VM distribution by ESXi Clusters\n",
    "\n",
    "This is orthogonal to the VM distribution analysis by `vCenters`, as a vCenter is likely to contain multiple `clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'inscope_vinfo_df' only includes in-scope VMs\n",
    "inscope_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Pivot the VM information on the clusters\n",
    "inscope_cluster_pivot = inscope_vinfo_df.pivot_table(index='Cluster',\n",
    "                                                      values='VM',\n",
    "                                                      aggfunc='count')  # Count VMs per cluster\n",
    "\n",
    "# Rename column for clarity\n",
    "inscope_cluster_pivot.rename(columns={'VM': 'VM_Count'}, inplace=True)\n",
    "\n",
    "# Sort by VM_Count in descending order\n",
    "inscope_cluster_pivot = inscope_cluster_pivot.sort_values(by='VM_Count', ascending=False)\n",
    "\n",
    "# Calculate the total number of VMs\n",
    "total_vms = int(inscope_cluster_pivot['VM_Count'].sum())  # Convert to Python int\n",
    "\n",
    "# Calculate the total number of clusters\n",
    "total_clusters = int(inscope_cluster_pivot.shape[0])  # Convert to Python int\n",
    "\n",
    "# Show only the top 10 clusters in the summary\n",
    "print(\"\\U0001F50D \"f'Distribution of VMs to Clusters (Top 10 by VM Count):')\n",
    "print(inscope_cluster_pivot.head(10).to_string(index=True))  # Shows only top 10 sorted\n",
    "print(f'\\n\\U0001F525 Total VMs across all clusters: {total_vms}')\n",
    "print(f'\\U0001F525 Total number of clusters: {total_clusters}')\n",
    "\n",
    "# Copy full results to clipboard for easy pasting into Excel\n",
    "inscope_cluster_pivot.to_clipboard(excel=True)\n",
    "\n",
    "# Create a pie chart (using only the top 10 clusters for clarity)\n",
    "top_10_pivot = inscope_cluster_pivot.head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_pivot['VM_Count'], labels=top_10_pivot.index, \n",
    "        autopct='%1.1f%%', startangle=140, explode=[0.05] * len(top_10_pivot))\n",
    "plt.title('\\n VM Distribution (Top 10 Clusters)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 10- VMs Categorized by Environment\n",
    "1. Create an Environment Column if it doesn't exist: This column will be based on the name of the ESXi cluster, which indicates the site location.\n",
    "3. Handle Unclassified Clusters: Any cluster names that do not have a environment category will be grouped as \"Unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Check if environment_summary exists, else create it\n",
    "if 'environment_summary' not in globals():\n",
    "    if 'filtered_vinfo_df' not in globals():\n",
    "        raise ValueError(\"Neither environment_summary nor filtered_vinfo_df is defined. Ensure the required data is available.\")\n",
    "    \n",
    "    # Ensure the \"Environment\" column exists and replace NaN or empty values with \"Unknown\"\n",
    "    if 'Environment' in filtered_vinfo_df.columns:\n",
    "        filtered_vinfo_df['Environment'] = filtered_vinfo_df['Environment'].fillna('Unknown')\n",
    "        filtered_vinfo_df.loc[filtered_vinfo_df['Environment'].astype(str).str.strip() == '', 'Environment'] = 'Unknown'\n",
    "    \n",
    "    # Create environment_summary from filtered_vinfo_df\n",
    "    environment_summary = filtered_vinfo_df.groupby('Environment').size().reset_index(name='VM_Count')\n",
    "\n",
    "# Remove total row if present\n",
    "environment_filtered = environment_summary[environment_summary['Environment'] != 'Total'].copy()\n",
    "\n",
    "# Convert VM_Count to numeric (if not already)\n",
    "environment_filtered.loc[:, 'VM_Count'] = pd.to_numeric(environment_filtered['VM_Count'])\n",
    "\n",
    "# Filter only In-Scope VMs (those that were not excluded)\n",
    "if 'filtered_vinfo_df' in globals():\n",
    "    in_scope_vms = filtered_vinfo_df[['VM', 'Environment']]\n",
    "    environment_filtered = environment_filtered[environment_filtered['Environment'].isin(in_scope_vms['Environment'].unique())]\n",
    "\n",
    "# If there are no VMs, skip plotting\n",
    "if not environment_filtered.empty:\n",
    "    # Extract environment names and VM counts\n",
    "    env_counts = dict(zip(environment_filtered['Environment'], environment_filtered['VM_Count']))\n",
    "    \n",
    "    # Explode effect (slightly separating each slice)\n",
    "    explode_values = [0.05] * len(env_counts)\n",
    "\n",
    "    # Plot the exploded pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(env_counts.values(), labels=env_counts.keys(), autopct='%1.1f%%', startangle=140, \n",
    "            colors=plt.cm.tab20.colors, explode=explode_values)\n",
    "    \n",
    "    plt.title('\\n VM Distribution by Environment')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 11- Summarize Operating Systems by Supported vs. Unsupported\n",
    "\n",
    "In this section, we provide a summary of the supported and unsupported operating systems for the in-scope vCenter instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure that 'ignored_vm_artifacts' represents only the VMs that were filtered out due to OS exclusion\n",
    "ignored_os_vms_df = ignored_vm_artifacts.copy()\n",
    "\n",
    "# Function to truncate OS names for better visualization\n",
    "def truncate_os_names(series):\n",
    "    return series.rename(lambda x: x if len(x) <= 40 else x[:37] + '...')\n",
    "\n",
    "# Group and count OS occurrences\n",
    "in_scope_os_counts = truncate_os_names(filtered_vinfo_df['Cleaned OS'].value_counts())\n",
    "out_of_scope_os_counts = truncate_os_names(ignored_os_vms_df['Cleaned OS'].value_counts())\n",
    "\n",
    "# Pie chart for In-Scope vs Out-of-Scope (OS Excluded) VMs\n",
    "plt.figure(figsize=(8, 8))\n",
    "scope_counts = [len(filtered_vinfo_df), len(ignored_os_vms_df)]\n",
    "labels = ['In-Scope OS\\'s', 'Out-of-Scope OS\\'s']\n",
    "colors = plt.cm.tab20.colors[:len(labels)]\n",
    "plt.pie(scope_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, explode=(0, 0.1))\n",
    "plt.title('Percentage of In-Scope vs Out-of-Scope OS\\'s')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for In-Scope OS Counts\n",
    "plt.figure(figsize=(14, 6))\n",
    "in_scope_os_counts.sort_values(ascending=False).plot(kind='bar', color='green')\n",
    "plt.title('In-Scope OS Counts')\n",
    "plt.xlabel('Operating System')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Out-of-Scope OS Counts (Only OS Excluded VMs)\n",
    "plt.figure(figsize=(14, 6))\n",
    "out_of_scope_os_counts.sort_values(ascending=False).plot(kind='bar', color='red')\n",
    "plt.title('Out-of-Scope OS Counts')\n",
    "plt.xlabel('Operating System')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 12- Migration Complexity by OS\n",
    "\n",
    "Categorized OS‚Äôs into\n",
    "\n",
    "* Easy\n",
    "* Medium\n",
    "* Hard\n",
    "* Database\n",
    "* Unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Categorize supported OS into difficulty levels\n",
    "os_difficulty_mapping = {\n",
    "    'Red Hat': 'Easy',\n",
    "    'Windows': 'Medium',\n",
    "    'Ubuntu': 'Hard',\n",
    "    'Suse': 'Hard',\n",
    "    'Oracle': 'Database',\n",
    "    'Microsoft SQL': 'Database'\n",
    "}\n",
    "\n",
    "# Ensure we only count OS instances from in-scope VMs\n",
    "in_scope_os_counts = filtered_vinfo_df['Cleaned OS'].value_counts()\n",
    "\n",
    "# Initialize difficulty counts\n",
    "difficulty_counts = {'Easy': 0, 'Medium': 0, 'Hard': 0, 'Database': 0, 'Unsupported': 0}\n",
    "\n",
    "# Map OS instances to difficulty levels\n",
    "for os_name, count in in_scope_os_counts.items():\n",
    "    difficulty = next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in os_name), 'Unsupported')\n",
    "    difficulty_counts[difficulty] += count\n",
    "\n",
    "# Identify unsupported OS instances within in-scope VMs\n",
    "unsupported_count = difficulty_counts['Unsupported']\n",
    "\n",
    "# Summary Section\n",
    "total_os_instances = sum(difficulty_counts.values())\n",
    "\n",
    "print(\"üîç Migration Complexity Summary\")\n",
    "print(f\"üßê Total OS Instances Analyzed: {total_os_instances}\")\n",
    "for level, count in difficulty_counts.items():\n",
    "    print(f\"  {level}: {count}\")\n",
    "print(f\"\\nüî¥ Unsupported OS Instances: {unsupported_count}\")\n",
    "\n",
    "# Print White Glove OS instances\n",
    "white_glove_os = [os for os in in_scope_os_counts.index if 'Oracle' in os or 'Microsoft SQL' in os]\n",
    "print(\"\\nüß§ White Glove OS Instances:\")\n",
    "for os in white_glove_os:\n",
    "    print(f\"üö® {os}: {in_scope_os_counts[os]}\")\n",
    "\n",
    "# Create a bar chart with automatic colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "difficulty_levels = list(difficulty_counts.keys())\n",
    "os_counts = list(difficulty_counts.values())\n",
    "\n",
    "# Generate a colormap automatically\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(difficulty_levels)))\n",
    "\n",
    "bars = plt.bar(difficulty_levels, os_counts, color=colors)\n",
    "plt.title('\\n Migration Complexity by OS')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('Number of OS Instances')\n",
    "\n",
    "# Add numeric labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, yval, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 13- Migration Complexity by Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82ddb2-4864-43d6-9f0b-13f4b4e368a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define disk size categories\n",
    "size_bins = [0, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Easy (0-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "\n",
    "# Categorize VMs by disk size\n",
    "filtered_vinfo_df['Disk Size Category'] = pd.cut(filtered_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Create summary\n",
    "disk_size_summary = filtered_vinfo_df['Disk Size Category'].value_counts().reindex(size_labels).reset_index()\n",
    "disk_size_summary.columns = ['Disk Size Category', 'VM Count']\n",
    "\n",
    "# Display summary\n",
    "print(\"üîç Migration Complexity by Disk:\")\n",
    "print(disk_size_summary.to_string(index=False))\n",
    "\n",
    "# Generate dynamic colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(size_labels)))\n",
    "\n",
    "# Plot bar chart with dynamic colors\n",
    "bars = plt.bar(disk_size_summary['Disk Size Category'], disk_size_summary['VM Count'], color=colors)\n",
    "\n",
    "plt.xlabel('Disk Size Category')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('\\n Migration Complexity by Disk Size')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add numeric labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, yval, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541e50e-6642-4a7a-a449-6b3dee2f2f69",
   "metadata": {},
   "source": [
    "### 14- Migration Complexity by Disk & OS\n",
    "\n",
    "This section computes disk & os into a complexity matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create explicit copy to avoid SettingWithCopyWarning\n",
    "filtered_vinfo_df = filtered_vinfo_df.copy()\n",
    "\n",
    "# Define updated disk size categories (removing 'Tiny' and adjusting 'Easy' to start from 0TB)\n",
    "size_bins = [0, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Easy (0-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "\n",
    "filtered_vinfo_df['Disk Size Category'] = pd.cut(\n",
    "    filtered_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels\n",
    ")\n",
    "\n",
    "# OS difficulty mapping\n",
    "os_difficulty_mapping = {\n",
    "    'Windows': 'Medium',\n",
    "    'Red Hat': 'Easy',\n",
    "    'Ubuntu': 'Hard',\n",
    "    'Suse': 'Hard',\n",
    "    'Oracle': 'Database',\n",
    "    'Microsoft SQL': 'Database'\n",
    "}\n",
    "\n",
    "# Map OS to difficulty level\n",
    "filtered_vinfo_df['OS Difficulty'] = filtered_vinfo_df['Cleaned OS'].apply(\n",
    "    lambda os: next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in os), 'Unsupported')\n",
    ")\n",
    "\n",
    "# Define complexity mapping function\n",
    "def determine_complexity(row):\n",
    "    os_difficulty = row['OS Difficulty']\n",
    "    disk_category = row['Disk Size Category']\n",
    "    \n",
    "    if os_difficulty == 'Database':\n",
    "        return 'White Glove'\n",
    "    \n",
    "    if os_difficulty == 'Easy':\n",
    "        if disk_category in ['Easy (0-10TB)', 'Medium (10-20TB)']:\n",
    "            return 'Easy'\n",
    "        return 'Hard' if disk_category != 'White Glove (>50TB)' else 'White Glove'\n",
    "    \n",
    "    if os_difficulty == 'Medium':\n",
    "        if disk_category in ['Easy (0-10TB)', 'Medium (10-20TB)']:\n",
    "            return 'Medium'\n",
    "        return 'Hard' if disk_category != 'White Glove (>50TB)' else 'White Glove'\n",
    "    \n",
    "    if os_difficulty == 'Hard':\n",
    "        if disk_category in ['Easy (0-10TB)', 'Medium (10-20TB)']:\n",
    "            return 'Medium'\n",
    "        return 'Hard' if disk_category != 'White Glove (>50TB)' else 'White Glove'\n",
    "    \n",
    "    return 'Unsupported'\n",
    "\n",
    "# Apply complexity mapping\n",
    "filtered_vinfo_df['Migration Complexity'] = filtered_vinfo_df.apply(determine_complexity, axis=1)\n",
    "\n",
    "# Define the desired order for sorting\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'White Glove', 'Unsupported']\n",
    "\n",
    "# Create a summary table counting VMs per complexity level and reordering it\n",
    "complexity_summary = filtered_vinfo_df['Migration Complexity'].value_counts().reindex(complexity_order, fill_value=0).reset_index()\n",
    "complexity_summary.columns = ['Migration Complexity', 'VM Count']\n",
    "\n",
    "# Display complexity summary\n",
    "print(\"üîç Migration Summary:\")\n",
    "print(complexity_summary.to_string(index=False, header=True))\n",
    "\n",
    "# Plot bar chart of migration complexity distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(complexity_summary['Migration Complexity'], complexity_summary['VM Count'], color=plt.cm.tab10.colors)\n",
    "plt.xlabel('Migration Complexity')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Migration Complexity Distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 15- Estimated Migration Time by OS\n",
    "\n",
    "FTE_COUNT can be changed to match the number of engineers that will be delivering the engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Constants\n",
    "migration_time_per_500gb = 110   # minutes (1 hour 50 minutes per 500GB)\n",
    "te_hours_per_day = 8             # 8 hours per day per FTE\n",
    "fte_count = 10                   # 10 FTEs available\n",
    "pmt_hours = 0.5                  # Post-migration troubleshooting time per VM in hours\n",
    "\n",
    "# Check if filtered_vinfo_df is defined\n",
    "try:\n",
    "    filtered_vinfo_df\n",
    "except NameError:\n",
    "    raise ValueError(\"filtered_vinfo_df is not defined. Ensure it is loaded before running this script.\")\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = {'Cleaned OS', 'Disk Size TB', 'VM', 'Cluster'}\n",
    "missing_columns = required_columns - set(filtered_vinfo_df.columns)\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in filtered_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Copy and preprocess DataFrame\n",
    "inscope_vinfo_df = filtered_vinfo_df.copy()\n",
    "inscope_vinfo_df['Cleaned OS'] = inscope_vinfo_df['Cleaned OS'].fillna(\"Unknown OS\").astype(str).str.strip()\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce').fillna(0)\n",
    "inscope_vinfo_df['VM'] = inscope_vinfo_df['VM'].fillna('Unknown VM')\n",
    "inscope_vinfo_df['Cluster'] = inscope_vinfo_df['Cluster'].fillna('').str.lower()\n",
    "\n",
    "# Define disk size classification\n",
    "size_bins = [0, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Easy (0-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "inscope_vinfo_df['Disk Size Category'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Generate supported OS list dynamically\n",
    "supported_os_counts = filtered_vinfo_df['Cleaned OS'].value_counts().index.tolist()\n",
    "inscope_vinfo_df['OS Support'] = inscope_vinfo_df['Cleaned OS'].apply(\n",
    "    lambda os: 'Supported' if any(supported_os in os for supported_os in supported_os_counts) else 'Not Supported'\n",
    ")\n",
    "\n",
    "# Complexity classification\n",
    "def classify_complexity(row):\n",
    "    cluster = str(row.get('Cluster', '')).lower()\n",
    "    disk_size_category = row.get('Disk Size Category', '')\n",
    "\n",
    "    if cluster.startswith('sql-') or 'oracle' in cluster:\n",
    "        return 'Database'\n",
    "\n",
    "    complexity_map = {\n",
    "        'Easy (0-10TB)': 'Easy',\n",
    "        'Medium (10-20TB)': 'Medium',\n",
    "        'Hard (20-50TB)': 'Hard',\n",
    "        'White Glove (>50TB)': 'White Glove'\n",
    "    }\n",
    "    return complexity_map.get(disk_size_category, 'Unknown')\n",
    "\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Sort complexity order\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'White Glove', 'Database']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(inscope_vinfo_df['Complexity'], categories=complexity_order, ordered=True)\n",
    "inscope_vinfo_df = inscope_vinfo_df.sort_values('Complexity')\n",
    "\n",
    "# Compute Migration Time\n",
    "inscope_vinfo_df['Migration Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * migration_time_per_500gb\n",
    ")\n",
    "\n",
    "# Compute Total Time (Migration + Post-Migration)\n",
    "pmt_minutes = pmt_hours * 60\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Migration Time (minutes)'] + pmt_minutes\n",
    "\n",
    "# Summarize data\n",
    "disk_classification_summary = inscope_vinfo_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk=('Disk Size TB', 'sum'),\n",
    "    Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Format Total_Disk to TB\n",
    "disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].apply(lambda x: f\"{x:.2f} TB\")\n",
    "\n",
    "# Compute Total Days and Weeks\n",
    "disk_classification_summary['Total_Days'] = disk_classification_summary['Total_Mig_Time_Minutes'] / (te_hours_per_day * 60 * fte_count)\n",
    "disk_classification_summary['Total_Weeks'] = disk_classification_summary['Total_Days'] / 5\n",
    "\n",
    "# Format output\n",
    "disk_classification_summary['Total_Days'] = disk_classification_summary['Total_Days'].apply(lambda x: f\"{x:.1f}\")\n",
    "disk_classification_summary['Total_Weeks'] = disk_classification_summary['Total_Weeks'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "# Add totals row\n",
    "totals_row = {\n",
    "    'Complexity': 'Total',\n",
    "    'OS Support': '',\n",
    "    'VM_Count': disk_classification_summary['VM_Count'].sum(),\n",
    "    'Total_Disk': f\"{disk_classification_summary['Total_Disk'].str.replace(' TB', '').astype(float).sum():.2f} TB\",\n",
    "    'Total_Days': f\"{disk_classification_summary['Total_Days'].astype(float).sum():.1f}\",\n",
    "    'Total_Weeks': f\"{disk_classification_summary['Total_Weeks'].astype(float).sum():.1f}\"\n",
    "}\n",
    "disk_classification_summary = pd.concat([disk_classification_summary, pd.DataFrame([totals_row])], ignore_index=True)\n",
    "disk_classification_summary = disk_classification_summary.drop(columns=['Total_Mig_Time_Minutes'])\n",
    "\n",
    "# Function to format the summary table\n",
    "def format_table(headers, rows):\n",
    "    horizontal_line = \"‚îÄ\"\n",
    "    vertical_line = \"‚îÇ\"\n",
    "    corner_tl, corner_tr = \"‚ï≠\", \"‚ïÆ\"\n",
    "    corner_bl, corner_br = \"‚ï∞\", \"‚ïØ\"\n",
    "    join_t, join_b, join_c = \"‚î¨\", \"‚î¥\", \"‚îº\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "    \n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "    \n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "# Convert DataFrame to formatted table\n",
    "headers = list(disk_classification_summary.columns)\n",
    "rows = disk_classification_summary.values.tolist()\n",
    "formatted_table = format_table(headers, rows)\n",
    "\n",
    "# Print formatted summary table\n",
    "print(f\"üîç Migraton Summary\")\n",
    "print(formatted_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc2e3b-eafe-4dc0-a4b8-734fe68b2ae5",
   "metadata": {},
   "source": [
    "### 16- Estimated Migration Time by Environment\n",
    "\n",
    "FTE_COUNT can be changed to match the number of engineers that will be delivering the engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f64331-b58c-477a-8a66-9b76f7011535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Constants\n",
    "MIGRATION_TIME_PER_500GB = 110   # minutes (1 hour 50 minutes per 500GB)\n",
    "TE_HOURS_PER_DAY = 8             # 8 hours per day per FTE\n",
    "FTE_COUNT = 10                   # 10 FTEs available\n",
    "PMT_HOURS = 0.5                  # Post-migration troubleshooting time per VM in hours\n",
    "PMT_MINUTES = PMT_HOURS * 60\n",
    "\n",
    "# Validate input DataFrame\n",
    "try:\n",
    "    filtered_vinfo_df\n",
    "except NameError:\n",
    "    raise ValueError(\"filtered_vinfo_df is not defined. Ensure it is loaded before running this script.\")\n",
    "\n",
    "REQUIRED_COLUMNS = {'Environment', 'Disk Size TB', 'VM'}\n",
    "missing_columns = REQUIRED_COLUMNS - set(filtered_vinfo_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Preprocess DataFrame\n",
    "inscope_vinfo_df = filtered_vinfo_df.copy()\n",
    "inscope_vinfo_df.fillna({'Environment': 'Unknown', 'VM': 'Unknown VM'}, inplace=True)\n",
    "inscope_vinfo_df['Environment'] = inscope_vinfo_df['Environment'].astype(str).str.strip()\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce').fillna(0)\n",
    "\n",
    "# Compute Migration and Total Time\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * MIGRATION_TIME_PER_500GB + PMT_MINUTES\n",
    ")\n",
    "\n",
    "# Summarize data\n",
    "environment_summary = inscope_vinfo_df.groupby('Environment', observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk=('Disk Size TB', 'sum'),\n",
    "    Total_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Compute Total Days and Weeks\n",
    "environment_summary['Total_Days'] = environment_summary['Total_Time_Minutes'] / (TE_HOURS_PER_DAY * 60 * FTE_COUNT)\n",
    "environment_summary['Total_Weeks'] = environment_summary['Total_Days'] / 5\n",
    "\n",
    "# Format output\n",
    "environment_summary['Total_Disk'] = environment_summary['Total_Disk'].map(lambda x: f\"{x:.2f} TB\")\n",
    "environment_summary['Total_Days'] = environment_summary['Total_Days'].map(lambda x: f\"{x:.1f}\")\n",
    "environment_summary['Total_Weeks'] = environment_summary['Total_Weeks'].map(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "# Add totals row\n",
    "totals_row = {\n",
    "    'Environment': 'Total',\n",
    "    'VM_Count': environment_summary['VM_Count'].sum(),\n",
    "    'Total_Disk': f\"{environment_summary['Total_Disk'].str.replace(' TB', '').astype(float).sum():.2f} TB\",\n",
    "    'Total_Days': f\"{environment_summary['Total_Days'].astype(float).sum():.1f}\",\n",
    "    'Total_Weeks': f\"{environment_summary['Total_Weeks'].astype(float).sum():.1f}\"\n",
    "}\n",
    "\n",
    "environment_summary = pd.concat([environment_summary, pd.DataFrame([totals_row])], ignore_index=True)\n",
    "environment_summary.drop(columns=['Total_Time_Minutes'], inplace=True)\n",
    "\n",
    "# Display summary\n",
    "display(environment_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5090-41a4-48c2-8f84-2afec0cf6089",
   "metadata": {},
   "source": [
    "### 17- Estimated Migration Time by vCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334fa83-e0f0-4c23-b4df-af8f97541f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "fte_hours_per_day = 8  # 8 hours per day per FTE\n",
    "fte_count = 10         # 10 FTEs available\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'Complexity', 'OS Support', 'VM', 'Disk Size TB', 'Total Time (minutes)', 'vCenter'}\n",
    "missing_columns = required_columns - set(inscope_vinfo_df.columns)\n",
    "\n",
    "# Handle missing columns\n",
    "if 'OS Support' in missing_columns:\n",
    "    inscope_vinfo_df['OS Support'] = 'Unknown'  # Defaulting to 'Unknown'\n",
    "\n",
    "# If 'Complexity' is missing, classify based on disk size or other available data\n",
    "if 'Complexity' in missing_columns:\n",
    "    # Define disk size classification\n",
    "    size_bins = [0, 10, 20, 50, float('inf')]\n",
    "    size_labels = ['Easy', 'Medium', 'Hard', 'White Glove']\n",
    "    inscope_vinfo_df['Disk Size Category'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "    # Define complexity based on disk size and vCenter name\n",
    "    def classify_complexity(row):\n",
    "        cluster = str(row.get('vCenter', '')).lower()\n",
    "        disk_size_category = row.get('Disk Size Category', '')\n",
    "\n",
    "        if cluster.startswith('sql-') or 'oracle' in cluster:\n",
    "            return 'Oracle-DBs' if 'oracle' in cluster else 'MSSQL-DBs'\n",
    "\n",
    "        complexity_map = {\n",
    "            'Easy': 'Easy',\n",
    "            'Medium': 'Medium',\n",
    "            'Hard': 'Hard',\n",
    "            'White Glove': 'White Glove'\n",
    "        }\n",
    "        return complexity_map.get(disk_size_category, 'Tiny')  # Default to 'Tiny'\n",
    "\n",
    "    inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Recheck if any required columns are missing after handling defaults\n",
    "missing_columns = required_columns - set(inscope_vinfo_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in inscope_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Define complexity order\n",
    "complexity_order = ['Tiny', 'Easy', 'Medium', 'Hard', 'White Glove', 'Oracle-DBs', 'MSSQL-DBs']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(\n",
    "    inscope_vinfo_df['Complexity'],\n",
    "    categories=complexity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Get unique vCenters\n",
    "vcenters = inscope_vinfo_df['vCenter'].unique()\n",
    "\n",
    "# Dictionary to store summaries for each vCenter\n",
    "vcenter_summaries = {}\n",
    "\n",
    "# Function to format the table output\n",
    "def format_table(headers, rows):\n",
    "    horizontal_line = \"‚îÄ\"\n",
    "    vertical_line = \"‚îÇ\"\n",
    "    corner_tl, corner_tr = \"‚ï≠\", \"‚ïÆ\"\n",
    "    corner_bl, corner_br = \"‚ï∞\", \"‚ïØ\"\n",
    "    join_t, join_b, join_c = \"‚î¨\", \"‚î¥\", \"‚îº\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "    \n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "    \n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "# Process each vCenter separately\n",
    "for vcenter in vcenters:\n",
    "    vcenter_df = inscope_vinfo_df[inscope_vinfo_df['vCenter'] == vcenter]\n",
    "    disk_classification_summary = vcenter_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "        VM_Count=('VM', 'count'),\n",
    "        Total_Disk=('Disk Size TB', 'sum'),\n",
    "        Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Ensure all categories exist\n",
    "    for complexity in complexity_order:\n",
    "        for os_support in ['Supported', 'Not Supported', 'Unknown']:\n",
    "            if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                    (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "                disk_classification_summary = pd.concat([\n",
    "                    disk_classification_summary,\n",
    "                    pd.DataFrame({\n",
    "                        'Complexity': [complexity],\n",
    "                        'OS Support': [os_support],\n",
    "                        'VM_Count': [0],\n",
    "                        'Total_Disk': [0.0],\n",
    "                        'Total_Mig_Time_Minutes': [0]\n",
    "                    })\n",
    "                ], ignore_index=True)\n",
    "    \n",
    "    # Remove rows where VM_Count is zero\n",
    "    disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "    \n",
    "    # Set complexity order\n",
    "    disk_classification_summary['Complexity'] = pd.Categorical(\n",
    "        disk_classification_summary['Complexity'], categories=complexity_order, ordered=True\n",
    "    )\n",
    "    disk_classification_summary = disk_classification_summary.sort_values('Complexity')\n",
    "\n",
    "    # Compute total migration time and disk\n",
    "    total_disk_tb_numeric = disk_classification_summary['Total_Disk'].astype(float).sum()\n",
    "    total_mig_time_minutes = disk_classification_summary['Total_Mig_Time_Minutes'].sum()\n",
    "\n",
    "    # Format columns\n",
    "    disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / 60:,.1f}h\"\n",
    "    )\n",
    "    disk_classification_summary['Days_Per_FTEs'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    )\n",
    "\n",
    "    disk_classification_summary['VM_Count'] = disk_classification_summary['VM_Count'].astype(int).apply(lambda x: f\"{x:,}\")\n",
    "    disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].astype(float).apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "    # Add totals row\n",
    "    totals_row = {\n",
    "        'Complexity': 'Totals',\n",
    "        'OS Support': '',\n",
    "        'VM_Count': f\"{disk_classification_summary['VM_Count'].astype(str).replace(',', '', regex=True).astype(int).sum():,}\",\n",
    "        'Total_Disk': f\"{total_disk_tb_numeric:,.0f}\",\n",
    "        'Formatted_Mig_Time': f\"{total_mig_time_minutes / 60:,.1f}h\",\n",
    "        'Days_Per_FTEs': f\"{total_mig_time_minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    }\n",
    "    disk_classification_summary = pd.concat([\n",
    "        disk_classification_summary, pd.DataFrame([totals_row])\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    vcenter_summaries[vcenter] = disk_classification_summary\n",
    "    print(f\"\\nüîç Migration Summary for vCenter: {vcenter}\")\n",
    "    headers = [\"Complexity\", \"OS Support\", \"VM Count\", \"Total Disk (TB)\", \"Total Migration Time\", \"Total Days\"]\n",
    "    rows = disk_classification_summary[['Complexity', 'OS Support', 'VM_Count', 'Total_Disk', 'Formatted_Mig_Time', 'Days_Per_FTEs']].values.tolist()\n",
    "    print(format_table(headers, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977fd21-5e69-4260-9664-1d0d99b9ebb9",
   "metadata": {},
   "source": [
    "### 18- Estimated Migration Time (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9bb6a-587e-4c7b-898d-098d91cbea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate global totals across all vCenters\n",
    "global_total_mig_time = sum(\n",
    "    summary['Total_Mig_Time_Minutes'].sum() for summary in vcenter_summaries.values()\n",
    ")\n",
    "global_total_days = global_total_mig_time / (fte_hours_per_day * 60 * fte_count)\n",
    "\n",
    "# Calculate total weeks (assuming each week has 5 workdays)\n",
    "total_global_weeks = global_total_days / 5\n",
    "\n",
    "# Ensure all values are correctly formatted\n",
    "print(f\"‚úÖÔ∏è Global Total Migration Time: {global_total_mig_time / 60:,.1f}h\")\n",
    "print(f\"‚úÖÔ∏è Global Total Days: {global_total_days:,.1f}\")\n",
    "print(f\"‚úÖÔ∏è Global Total Weeks: {total_global_weeks:,.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
